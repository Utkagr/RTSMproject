---
title: "Online Buzz Prediction on Twitter"
output: github_document
---
```{r include=FALSE}
tdata <- read.csv("/home/utkarsh/rtsm/regression/Twitter/Twitter.data")
names(tdata) <- c("NCD_0","NCD_1","NCD_2","NCD_3","NCD_4","NCD_5","NCD_6",
                  "AI_0","AI_1","AI_2","AI_3","AI_4","AI_5","AI_6",
                  "AS(NA)_0","AS(NA)_1","AS(NA)_2","AS(NA)_3","AS(NA)_4","AS(NA)_5","AS(NA)_6",
                  "BL_0","BL_1","BL_2","BL_3","BL_4","BL_5","BL_6",
                  "NAC_0","NAC_1","NAC_2","NAC_3","NAC_4","NAC_5","NAC_6",
                  "AS(NAC)_0","AS(NAC)_1","AS(NAC)_2","AS(NAC)_3","AS(NAC)_4","AS(NAC)_5","AS(NAC)_6",
                  "CS_0","CS_1","CS_2","CS_3","CS_4","CS_5","CS_6",
                  "AT_0","AT_1","AT_2","AT_3","AT_4","AT_5","AT_6",
                  "NA_0","NA_1","NA_2","NA_3","NA_4","NA_5","NA_6",
                  "ADL_0","ADL_1","ADL_2","ADL_3","ADL_4","ADL_5","ADL_6",
                  "NAD_0","NAD_1","NAD_2","NAD_3","NAD_4","NAD_5","NAD_6","MNAD")
ndata <- NULL
table_res<- NULL
train <- NULL
test <- NULL
model <- NULL
bmodel <- NULL
ndata$NCD <- rowSums(tdata[c(1:7)])
ndata$AI <- rowSums(tdata[c(8:14)])
ndata$AS_NA <- rowSums(tdata[c(15:21)])
ndata$BL <- rowSums(tdata[c(22:28)])
ndata$NAC <- rowSums(tdata[c(29:35)])
ndata$AS_NAC <- rowSums(tdata[c(36:42)])
ndata$CS <- rowSums(tdata[c(43:49)])
ndata$AT <- rowSums(tdata[c(50:56)])
ndata$NAO <- rowSums(tdata[c(57:63)])
ndata$ADL <- rowSums(tdata[c(64:70)])
ndata$NAD <- rowSums(tdata[c(71:77)])
ndata$MNAD <- tdata$MNAD
ndata <- data.frame(ndata)

set.seed(100)
ndata <- ndata[1:198373,]
ndata <- scale(ndata)
ndata <- data.frame(ndata)

#split data
smp_size <- floor(0.70 * nrow(ndata))
train <- ndata[1:smp_size,]
test <- ndata[(1+smp_size):(nrow(ndata)),]

```
Let's take a look on the data.
```{r echo=FALSE}
train[1:5,]
```
## Attributes

    -- Number of Created Discussions (NCD)

       -- Type : Numeric, integers only 
       -- Description : This feature measures the number of discussions created 
          at time step t and involving the instance's topic.

    -- Author Increase (AI)

       -- Type : Numeric, integers only 
       -- Description : This featurethe number of new authors interacting on
          the instance's topic at time t (i.e. its popularity)
    
    -- Attention Level (measured with number of authors) (AS(NA)) 
    
       -- Type : Numeric, real in [0,1]
       -- Description : This feature is a measure of the attention payed to a 
          the instance's topic on a social media.
       
    -- Burstiness Level (BL) (columns [21,27])

       -- Type : Numeric, defined on [0,1] 
       -- Description : The burstiness level for a topic z at a time t is 
          defined as the ratio of ncd and nad
          
    -- Number of Atomic Containers (NAC)

       -- Type : Numeric, integer
       -- Description : This feature measures the total number of atomic 
          containers generated through the whole social media on the instance's topic until time t.

    -- Attention Level (measured with number of contributions) (AS(NAC)) 

       -- Type : Numeric, real in [0,1]
       -- Description : This feature is a measure of the attention payed to a 
          the instance's topic on a social media.

    -- Contribution Sparseness (CS)

       -- Type : Numeric, real in [0,1] 
       -- Description : This feature is a measure of spreading of contributions
          over discussion for the instance's topic at time t.
          
    -- Author Interaction (AT)

       -- Type : Numeric, integer.
       -- Description : This feature measures the average number of authors
          interacting on the instance's topic within a discussion.

    -- Number of Authors (NA)

       -- Type : Numeric, integer.
       -- Description : This feature measures the number of authors interacting
          on the instance's topic at time t.
          
    -- Average Discussions Length (ADL)

       -- Type : Numeric, real.
       -- Description : This feature directly measures the average length of a 
          discussion belonging to the instance's topic.

    -- Average Discussions Length (NAD)

       -- Type : Numeric, integer.
       -- Description : This features measures the number of discussions
          involving the instance's topic until time t.

## Multicollinearity Diagnostics

#Correlation matrix
```{r echo = FALSE}
cor(train[-c(12)])
```
Let's take a look at the correlation plots for better understanding of the data.

```{r}
library(corrplot)
m <- cor(train[-c(12)])
corrplot(m,order = "hclust",addrect=3)
```

We can infer from the above plot that most of the features are highly correlated.<br>
The 3 rectangles in the plot mark the territory of highly correlated features mentioned below.<br>
-NCD AI AS_NA NAC AS_NAC NAO NAD <br>
-CS BL<br>
-AT ADL<br>
We can also take a look at the VIF values.
```{r include=FALSE}
library(usdm)
```

```{r echo = FALSE}
vif(train[-c(12)])
```
An ideal VIF value is 1.<br>
So,any feature having vif value close to 1 is significant.<br>
A range of 1-5 for the VIF value is preferred.<br>
After 5, the feature is not considered significant.<br>
As we can see, our features lie nowhere near VIF value of 1.<br>
This suggests that our features are highly correlated as we have seen in earlier plot as 
well.<br>
So,we need to eliminate features which donot add much variance to the data.<br>
For that, let's do principal component analysis which projects the features into the direction of maximum variance.<br>
#PCA analysis
For PCA,data needs to be:<br>
  - scaled
  - all numeric
  - no missing values

```{r}
features <- train[-c(12)]
prin_comp <- prcomp(features,scale. = T)
prin_comp$x[1:5,]
dim(prin_comp$x)

```
Now,letâ€™s plot the resultant principal components.
```{r}
biplot(prin_comp,scale = 0)
```
We can see that PC1 and PC2 both come from some features which are marked red.<br>
 i.e. PC1 = a1x1 + a2x2 + a3x3 (say)<br>
 and  PC2 = b1x4 + b2x5 + b3x6 (say)<br>
Basically,PCA's are the resulatant of the correlated features.

Now,let's calculate the variance contribution of every principal component as we aim to find the components which explain the maximum variance.<br>
This is because, we want to retain as much information as possible using these 
components.<br>
So, higher is the explained variance, higher will be the information 
contained in those components.

```{r}
std_dev <- prin_comp$sdev
pr_var <- std_dev^2
prop_varex <- (pr_var/sum(pr_var))*100
prop_varex

```
As we can see,PCA1 contributes app. 59% of the variance and hence is the most important feature.<br>
For more meaningful inference,we make a scree plot.<br>
A scree plot is used to access components or factors which explains the most of variability in the data.<br>
It represents values in descending order.
```{r}
plot(prop_varex, xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     type = "b")
```
Here we can see that 6 components approximately 98% variance in the dataset.<br>
For the confirmation check,let's plot a cumulative variance plot.
```{r}
plot(cumsum(prop_varex), xlab = "Principal Component",
     ylab = "Cumulative Proportion of Variance Explained",
     type = "b")
```

The graph clearly shows that we should select 6 features which explains almost 98% of the data.<br>
Hence,we will choose 6 variables from PC1 to PC6 for our model and continue further.
```{r}
pca_data <- data.frame(prin_comp$x[,c(1:6)])
pca_data$MNAD <- train$MNAD
pca_data[1:5,]
```
Now,our training data looks like above.<br>
Let's do the correlation and VIF analysis on this.
```{r}
cor(pca_data[-c(7)])
```
```{r}
vif(pca_data[-c(7)])
```
As we can see, VIF values are close to 1 and correlation matrix also shows that features are independent.<br>
Now, our features are scaled and independent.<br>
Let's apply a regression model now.

##General multiple regression model

```{r}
model=lm(MNAD~.,pca_data)
summary(model)
```
Here,we get an adjusted- R-square value of 0.8547 and all the features are significant as well.<br>
Now, our model seems good.<br>
But, we can still drop a variable to check if our model improves or not.<br>
For that,we can use methods of regression for propagating back-and-forth.

# METHODS OF REGRESSION
1.Forward Selection Method<br>
2.Backward Elimination Method<br>
3.Stepwise Method

For our analysis,we have considered Backward elimination method.
```{r}
bmodel <- step(model, direction = "backward", trace=TRUE )
summary(bmodel)

```
As we can see,no features are rejected in the steps of backward-elimination method.<br>
Thus,we can conclude that our model will include all of the 6 variables.<br>
Now,let's take a look at the coefficients and confidence intervals of the features.

#Coefficients
```{r echo=FALSE}
coefficients(model) # model coefficients

```
#Confidence Intervals
```{r echo=FALSE}
confint(model) # CIs for model parameters

```
As we can see,all the coefficients lie in the confidence interval.

Hence,our equation is
# MNAD = 0.39147901*PC1 + 0.04093860 * PC2 + 0.02838228 * PC3 + 0.32418139 * PC4 + 0.26756121 * PC5 + 0.12913215 * PC6 + 0.01412517

Now,let's get to the prediction part.<br>
For prediction,we should not use PCA on train and test separately as their variance is unequal which will result in different vector directions.<br>
Also,we should not combine the training and test set as we donot want our test set to be used in model building.<br>
What we can do is use predict function as shown below.
```{r}
pc_data_test <- predict(prin_comp,test[-c(12)])
pc_data_test <- as.data.frame(pc_data_test)
pc_data_test <- pc_data_test[c(1:6)]
pc_data_test[1:5,]

```
Now,let's predict on the above dataset.
```{r}
pc_data_test$predicted_MNAD <- predict(model,pc_data_test)
pc_data_test$MNAD <- test$MNAD
pc_data_test$residuals <- pc_data_test$MNAD-pc_data_test$predicted_MNAD
pc_data_test[1:5,7:9]
```
We can see that residuals are very less which explains the accuracy of our model.<br>
But,we have to do residual analysis for more accurate results.<br>
Let's calculate the variance of residuals.
```{r}
Se_sq <- sum((pc_data_test$residuals)^2)
n <- nrow(test)
p <- 7
var_res <- Se_sq/(n-p)
std_res <- sqrt(var_res)
std_res
```
Now,let's do Standardized Residual Analysis.
```{r}
pc_data_test$stand_residuals <- pc_data_test$residuals/std_res
mean(pc_data_test$stand_residuals)
var(pc_data_test$stand_residuals)
```
We can see that mean is close to 0 and variance is close to 1.
Let's plot them.
```{r}
plot(pc_data_test$stand_residuals,main = 'Standardized Residuals plot',
     xlab='Observation no.',ylab = 'residuals')
abline(h=5,untf = FALSE)
abline(h=-5,untf = FALSE)
```

Now,we can say that a large standaradized residual(>5,say) potentially indicates an outlier.
```{r}
qqnorm(pc_data_test$stand_residuals,
         ylab="Residuals",
         xlab="Normal Scores",main="Normal Q-Q plot") 
qqline(pc_data_test$stand_residuals)

```

Now,let's plot normal probability plot.
```{r}
y <- (as.numeric(rownames(pc_data_test))-0.5)/nrow(pc_data_test)
x <- sort(pc_data_test$stand_residuals,decreasing = FALSE)
plot(x,y,
     xlab = 'Standardized residuals',
     ylab = 'Probability')
```
This plot is not ideal as it contains a lot of outliers.<br>
Let's take a closer look.
```{r}
plot(x,y,
     xlab = 'Standardized residuals',
     ylab = 'Probability',
     xlim = c(-1,1))
```
This suggests a light-tailed distribution.
